{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter Server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/bin/jupyter-notebook\", line 5, in <module>\n",
      "\u001b[1;31m    from notebook.app import main\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/notebook/app.py\", line 17, in <module>\n",
      "\u001b[1;31m    from jupyter_server.serverapp import flags\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jupyter_server/serverapp.py\", line 39, in <module>\n",
      "\u001b[1;31m    from jupyter_events.logger import EventLogger\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jupyter_events/__init__.py\", line 3, in <module>\n",
      "\u001b[1;31m    from .logger import EVENTS_METADATA_VERSION, EventLogger\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jupyter_events/logger.py\", line 14, in <module>\n",
      "\u001b[1;31m    from jsonschema import ValidationError\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jsonschema/__init__.py\", line 13, in <module>\n",
      "\u001b[1;31m    from jsonschema._format import FormatChecker\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jsonschema/_format.py\", line 11, in <module>\n",
      "\u001b[1;31m    from jsonschema.exceptions import FormatError\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/jsonschema/exceptions.py\", line 15, in <module>\n",
      "\u001b[1;31m    from referencing.exceptions import Unresolvable as _Unresolvable\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/referencing/__init__.py\", line 5, in <module>\n",
      "\u001b[1;31m    from referencing._core import Anchor, Registry, Resource, Specification\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/referencing/_core.py\", line 9, in <module>\n",
      "\u001b[1;31m    from rpds import HashTrieMap, HashTrieSet, List\n",
      "\u001b[1;31m  File \"/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/rpds/__init__.py\", line 1, in <module>\n",
      "\u001b[1;31m    from .rpds import *\n",
      "\u001b[1;31mImportError: dlopen(/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/rpds/rpds.cpython-312-darwin.so, 0x0002): tried: '/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/rpds/rpds.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/rpds/rpds.cpython-312-darwin.so' (no such file), '/Users/mac/Documents/Purwadhika/.venv/lib/python3.12/site-packages/rpds/rpds.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import folium\n",
    "from scipy.stats import normaltest, chi2_contingency, mannwhitneyu, ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Problem that need to be answered in this analysis**\n",
    "\n",
    "\n",
    "In this capstone project there are some question that needs to be answered:\n",
    "<br><br>\n",
    "\n",
    "1. Driver and fleet optimization : How to optimize fleet operations by increasing efficiency and reducing idle time\n",
    "\n",
    "2. How to identify the busiest times and predict the amount of demand at a given time When and where taxi demand increases\n",
    "\n",
    "3. Payment Preferences and Driver Income Impact Which payment methods are most popular, and how do they impact driver income\n",
    "\n",
    "4. Relationship between Trip Fare and Distance Is there a correlation between trip distance and fare amount?\n",
    "\n",
    "5. Trip Duration and Zone Analysis Which zones (PULocationID, DOLocationID) experience the longest travel times?\n",
    "6. Impact of Surcharges on Total Fare How do surcharges (MTA taxes, surcharges, tolls) affect total fare?\n",
    "7. Detecting Fraudulent Trips Are there any unusual trips that could indicate data entry errors or fraud?\n",
    "8. Comparison between vendor performance Is there a difference between the average trip distance, time and cost of vendor 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **The method**\n",
    "\n",
    "The steps applied in this project will include but are not limited to these highlighted below.\n",
    "\n",
    "1. Data Preprocessing and Cleaning\n",
    "- Handle missing values \n",
    "- Convert date-time fields into an appropriate format and then extract relevant features from the date: hour, weekday, month.\n",
    "- Handle outliers in trip_distance and fare_amount-for instance, extremely high values for fare_amount.\n",
    "- Map location IDs to known NYC boroughs/zones for better insights.\n",
    "2. Exploratory Data Analysis\n",
    "- Find peak hours, days of the week, and seasonal trends.\n",
    "- Map hot zones for pickups and drop-offs based on PULocationID and DOLocationID.\n",
    "- Look at fare vs. trip distance based on Rate Codes.\n",
    "- Check different usage patterns among different payment methods.\n",
    "3. Feature Engineering\n",
    "New features to be created are as follows:\n",
    "- trip duration : Time difference between dropoff and pickup.\n",
    "- Trip Speed: The trip distance divided by the trip time .\n",
    "- Rush Hour: Provides an indicator flag on whether it is rush hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the data into dataframe format called df_raw\n",
    "df_raw= pd.read_csv('D:/PURWADHIKA/New York City Taxi and Limousine Trip Record/NYC TLC Trip Record.csv')\n",
    "display(df_raw.head(),df_raw.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the df raw into df. later on df data will be the base dataframe to modify/cleaned\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 1: Data Understanding:**\n",
    "\n",
    "In this step, we need to understand the dimension of the data, number of duplicate value, number of null value and the statistic definition of the data for example mean, quantile , etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of row and column in the dataset df are {df.shape}')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe(), df.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique data for every column to understand better the data\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "\n",
    "listItem = []\n",
    "for col in df.columns : listItem.append( [col, df[col].nunique(), df[col].unique()])\n",
    "\n",
    "tabel1Desc = pd.DataFrame(columns=['Column Name', 'Number of Unique', 'Unique Sample'],data=listItem)\n",
    "tabel1Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the duplicate of the data\n",
    "duplicateRows = df[df.duplicated()]\n",
    "print(duplicateRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1: Data Understanding - Result**\n",
    "\n",
    "1. Number of row and column in the dataset df are (68211, 20)\n",
    "2. ehail_fee column has all null value. This column will be dropped because there's no such a pattern that need to be investigated further\n",
    "\n",
    "3. There are nan value in some of the column with percentage missing data as follow\n",
    "<br>\n",
    "    - store_and_fwd_flag 6.339154 %\n",
    "    - RatecodeID 6.339154 %\n",
    "    - passenger_count            6.339154  %\n",
    "    - payment_type               6.339154 %\n",
    "    - trip_type                  6.353814 %\n",
    "    - congestion_surcharge       6.339154 %\n",
    "    <br>\n",
    "    These data needs to be further investigated so that the treament of this nan value can be decided. The investigation including pattern investigation\n",
    "\n",
    "4. There are some unappropriate deffinition of data types. Here are 3 data types that  needs to be  adjusted in order \n",
    "    - lpep_pickup_datetime and lpep_dropoff_datetime:\n",
    "        <br>\n",
    "        Current Type: object (possibly string), Expected Type: datetime\n",
    "        This data will be converted into  datetime format, because they represent date and time information. This conversion will support other further anlysis such as  calculating trip duration, filtering by specific time ranges, and other time-related analyses.\n",
    "\n",
    "    - store_and_fwd_flag:\n",
    "        <br>\n",
    "        Current Type: object (posibly string with values like \"Y\" and \"N\")\n",
    "        Expected Type: category (or a binary int or bool)\n",
    "        This data will be converted into a category so that will reduce memory usage, and it allows for efficient filtering.\n",
    "\n",
    "5.  Found negative value on Improvement_surcharge. Improvement surcharge is basically a fixed addition-almost always $0.30-per-trip charge to help pay for improvements and infrastructure. It's added on top of the fare and does not change based on distance or time of the trip.It is not logicall when it has negative values in this column, it may indicate Data Entry Errors or Data Processing Problems where there could be an error in importing or processing data, especially where some computation or offset would have applied. This data should be handled later.\n",
    "\n",
    "6. Found negative value on mta_tax: mta_tax is a constant item that is added on each taxi fare in New York City and the money from which is used to benefit Metropolitan Transportation Authority. It should be positive or zero since it is an addition to fare. The negative value probably caused by an error in data entry or processing, such as improper adjustment or transformation of data. The pattern of this data needs to be observed later\n",
    "\n",
    "7. Found negative value on sxtra: There is no definition of extra on PDF docs. On many cases, The extra field to indicates more surcharges such as rush hour fees or overnight charges. Similar to the mta_tax, these are the costs that come on top of the fare due to particular conditions-peak hours in this case-and would always be positive or zero but not negative. The pattern of this data needs to be observed later\n",
    "\n",
    "8. There is different category that can be found on RatecodeId. On the data definition, the RatecodeID defined as follow : \n",
    "<br> \n",
    "    1=Standardrate; 2=JFK; 3=Newark; 4=Nassau or Westchester; 5=Negotiated fare; 6=Group ride\n",
    "<br><br>\n",
    "    In reality, however, RatecodeID for the real data contains the following values: [1.0, 5.0, 4.0, 3.0, 2.0, 99.0, nan]. Notably, the value 99.0 seems a bit misplaced while considering the categories defined earlier, as this should be 6 for group rides; in this case, further investigation might be needed.\n",
    "\n",
    "9. There is no duplicated row on the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 2: Missing Value**\n",
    "<br>\n",
    "In this step, the pattern of the missing values are observed and the treatment of each missing values is decided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1: ehail_fee**\n",
    "<br>\n",
    "Drop the column of ehail_fee and observe the dimension of the data after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to drop the data\n",
    "df = df.drop(columns=['ehail_fee'])\n",
    "#print the information of dataframe after dropping the data\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Nan value investigation and treatment**\n",
    "<br>\n",
    "These datas has same percentage of missing values.\n",
    "- store_and_fwd_flag 6.339154 %\n",
    "- RatecodeID 6.339154 %\n",
    "- passenger_count            6.339154  %\n",
    "- payment_type               6.339154 %\n",
    "- trip_type                  6.353814 %\n",
    "- congestion_surcharge       6.339154 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data to understand the distribution of the data\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df[['VendorID','RatecodeID','PULocationID','DOLocationID','passenger_count','trip_distance','fare_amount','extra','mta_tax','tip_amount','tolls_amount','ehail_fee','improvement_surcharge','total_amount','payment_type','trip_type','congestion_surcharge']].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above showing that the missing value of the data concetrated at lowest row of the data. \n",
    "<br>\n",
    "The other pattern of the missing value will be deeper investigated such as : are these missing value corralated to certain vendor , ratecodeid etc\n",
    "<br>\n",
    "To better understand the data the modification of data type lpep_pickup_datetime and lpep_dropoff_datetime are needed so that this data will give better information such as date and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data types to date and time types\n",
    "df['lpep_pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "df['lpep_dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features for the dates so that we can understand detail and creates grouping based on the dates\n",
    "df['pickup_hour'] = pd.to_datetime(df['lpep_pickup_datetime']).dt.hour\n",
    "df['pickup_day'] = pd.to_datetime(df['lpep_pickup_datetime']).dt.dayofweek\n",
    "df['pickup_date'] = df['lpep_pickup_datetime'].dt.date\n",
    "df['pickup_month'] = df['lpep_pickup_datetime'].dt.month      \n",
    "df['is_weekend'] = df['pickup_day'].apply(lambda x: 1 if x >= 5 else 0)  # Weekend indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().mean() * 100\n",
    "print(\"Percentage of missing values in each column:\\n\", missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Analyze patterns in missing data\n",
    "\n",
    "# Vendor-based missing data analysis\n",
    "vendor_missing_data = df.groupby('VendorID').apply(lambda x: x.isnull().mean() * 100, include_groups=False)\n",
    "print(\"Missing data percentage by VendorID:\\n\", vendor_missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values by VendorID\n",
    "vendor_missing_data.T.plot(kind='bar', figsize=(10, 6))\n",
    "plt.ylabel(\"Percentage of Missing Values\")\n",
    "plt.title(\"Missing Values by Feature and VendorID\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data percentage by pickup hour\n",
    "hourly_missing_data = df.groupby('pickup_hour').apply(lambda x: x.isnull().mean() * 100, include_groups=False)\n",
    "print(\"Missing data percentage by Pickup Hour:\\n\", hourly_missing_data)\n",
    "\n",
    "# Plot missing data by hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(hourly_missing_data.T, cmap=\"YlGnBu\", annot=True, fmt=\".1f\")\n",
    "plt.title(\"Missing Values by Hour of Day\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location-based missing data analysis: check if missing data is location-related\n",
    "# Calculate missing data percentage by pickup location ID\n",
    "location_missing_data = df.groupby('PULocationID').apply(lambda x: x.isnull().mean() * 100, include_groups=False)\n",
    "\n",
    "print(\"Missing data percentage by Pickup Location ID:\\n\", location_missing_data)\n",
    "\n",
    "# Plot missing data by location\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(location_missing_data.T, cmap=\"YlGnBu\", annot=False)\n",
    "plt.title(\"Missing Values by Pickup Location\")\n",
    "plt.xlabel(\"Pickup Location ID\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values grouped by 'store_and_fwd_flag'\n",
    "store_and_fwd_missing_data = df.groupby('store_and_fwd_flag').apply(lambda x: x.isnull().mean() * 100)\n",
    "store_and_fwd_missing_data = store_and_fwd_missing_data.round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"Percentage of missing values grouped by store_and_fwd_flag:\\n\", store_and_fwd_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_and_fwd_missing_data.T.plot(kind='bar', figsize=(14, 7), colormap='viridis')\n",
    "plt.title(\"Percentage of Missing Values by Feature Grouped by 'store_and_fwd_flag'\")\n",
    "plt.ylabel(\"Percentage of Missing Values\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Store and Forward Flag\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: Check for other patterns in missing data\n",
    "\n",
    "# Calculate correlations between missing values to find any relationship patterns\n",
    "missing_corr = df.isnull().corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(missing_corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Between Missing Values of Different Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if shorter or longer trips have more missing values\n",
    "\n",
    "distance_missing = df.groupby(pd.cut(df['trip_distance'], bins=5), observed=True).apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "print(\"Missing data percentage by Trip Distance Bins:\\n\", distance_missing)\n",
    "\n",
    "# Plot missing data by trip distance bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(distance_missing.T, cmap=\"YlGnBu\", annot=True, fmt=\".1f\")\n",
    "plt.title(\"Missing Values by Trip Distance Bins\")\n",
    "plt.xlabel(\"Trip Distance Bin\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the extracted date and calculate missing values\n",
    "date_missing_data = df.groupby('pickup_date').apply(lambda x: x.isnull().mean() * 100, include_groups=False)\n",
    "\n",
    "\n",
    "# Results\n",
    "print(\"Percentage of missing values grouped by pickup date:\")\n",
    "print(date_missing_data)\n",
    "\n",
    "# Plot the data for better understanding\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(date_missing_data.T, cmap=\"YlGnBu\", annot=False, cbar_kws={'label': 'Percentage of Missing Values'})\n",
    "plt.title(\"Missing Values by Column Grouped by Pickup Date\")\n",
    "plt.xlabel(\"Pickup Date\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where Passenger_count is 0\n",
    "zero_passenger_data = df[df['passenger_count'] == 0]\n",
    "\n",
    "# Calculate the percentage of missing values in the specified columns for zero-passenger rows\n",
    "zero_passenger_missing = zero_passenger_data[['store_and_fwd_flag', 'RatecodeID', 'passenger_count', \n",
    "                                              'payment_type', 'trip_type', 'congestion_surcharge']].isnull().mean() * 100\n",
    "\n",
    "# Display the results\n",
    "print(\"Percentage of missing values for rows with Passenger_count = 0:\")\n",
    "print(zero_passenger_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values grouped by RatecodeID for the specified columns\n",
    "ratecode_missing_data = df.groupby('RatecodeID').apply(\n",
    "    lambda x: x[['store_and_fwd_flag', 'passenger_count', 'payment_type', 'trip_type', 'congestion_surcharge']].isnull().mean() * 100\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Percentage of missing values grouped by RatecodeID:\")\n",
    "print(ratecode_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall missing values grouped by RatecodeID\n",
    "overall_missing_data = df.groupby('RatecodeID').apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "# Display the results\n",
    "print(\"Overall percentage of missing values grouped by RatecodeID:\")\n",
    "print(overall_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall missing values grouped by RatecodeID without the DeprecationWarning\n",
    "overall_missing_data = df.groupby('RatecodeID', group_keys=False).apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "# Plotting overall missing values by RatecodeID\n",
    "plt.figure(figsize=(10, 6))\n",
    "overall_missing_data.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Overall Percentage of Missing Values by RatecodeID\")\n",
    "plt.ylabel(\"Percentage of Missing Values\")\n",
    "plt.xlabel(\"RatecodeID\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for RatecodeID equal to 99\n",
    "ratecode_99_data = df[df['RatecodeID'] == 99]\n",
    "# Calculate the number of missing values for each column\n",
    "missing_values_count_ratecode_99 = ratecode_99_data.isnull().sum()\n",
    "#  result\n",
    "print(\"Number of missing values for RatecodeID = 99:\")\n",
    "print(missing_values_count_ratecode_99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the detail investigation of the missing value grouped by certain variable it can be said that there is no pattern of the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.dropna(subset=['store_and_fwd_flag','RatecodeID','passenger_count', 'payment_type','trip_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_drop.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.isna().sum()/df_drop.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_drop.describe(), df_drop.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melakukan treatment terhadap nilai minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify negative values in the specified columns\n",
    "negative_improvement_surcharge = df[df['improvement_surcharge'] < 0]\n",
    "negative_mta_tax = df[df['mta_tax'] < 0]\n",
    "negative_extra = df[df['extra'] < 0]\n",
    "\n",
    "# Print the counts of negative values\n",
    "print(f\"Negative Improvement Surcharge Count: {negative_improvement_surcharge.shape[0]}\")\n",
    "print(f\"Negative MTA Tax Count: {negative_mta_tax.shape[0]}\")\n",
    "print(f\"Negative Extra Count: {negative_extra.shape[0]}\")\n",
    "\n",
    "print(\"Negative Improvement Surcharge Examples:\\n\", negative_improvement_surcharge.head())\n",
    "print(\"Negative MTA Tax Examples:\\n\", negative_mta_tax.head())\n",
    "print(\"Negative Extra Examples:\\n\", negative_extra.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# melakukan pembuatan boxplot untuk mengetahui outliers, distribution\n",
    "variables = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount']\n",
    "\n",
    "# Set up figure size and plot layout\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through variables to create subplots\n",
    "for i, var in enumerate(variables, 1):\n",
    "    plt.subplot(2, 2, i)  # Create 2x2 grid of plots\n",
    "    sns.boxplot(data=df, x=var)\n",
    "    plt.title(f'Boxplot of {var}')\n",
    "    plt.ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me;alilam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan pemisahan pickup\n",
    "df['pickup_hour'] = df['lpep_pickup_datetime'].dt.hour         # Extract hour\n",
    "df['pickup_day'] = df['lpep_pickup_datetime'].dt.dayofweek      # 0 = Monday, 6 = Sunday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['pickup_hour', 'pickup_day', 'pickup_month', 'is_weekend']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant numerical columns for correlation analysis\n",
    "numerical_cols = ['fare_amount', 'trip_distance', 'total_amount', 'tip_amount', 'passenger_count', 'congestion_surcharge']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Plot heatmap to visualize correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix for NYC Taxi Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pickup hour distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='pickup_hour', palette='Blues')\n",
    "plt.title('Number of Pickups by Hour')\n",
    "plt.show()\n",
    "\n",
    "# Plot pickup day distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='pickup_day', palette='Greens')\n",
    "plt.title('Number of Pickups by Day of the Week (0 = Monday, 6 = Sunday)')\n",
    "plt.show()\n",
    "\n",
    "# Monthly pickup trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='pickup_month', palette='Purples')\n",
    "plt.title('Number of Pickups by Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 pickup locations\n",
    "top_pickups = df['PULocationID'].value_counts().head(10)\n",
    "top_dropoffs = df['DOLocationID'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_pickups.index, y=top_pickups.values, palette='Reds')\n",
    "plt.title('Top 10 Pickup Locations')\n",
    "plt.xlabel('PULocationID')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_dropoffs.index, y=top_dropoffs.values, palette='Blues')\n",
    "plt.title('Top 10 Drop-off Locations')\n",
    "plt.xlabel('DOLocationID')\n",
    "plt.ylabel('Number of Drop-offs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df, x='trip_distance', y='fare_amount', hue='RatecodeID', palette='deep', alpha=0.5)\n",
    "plt.title('Fare Amount vs. Trip Distance by Rate Code')\n",
    "plt.xlabel('Trip Distance (miles)')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.legend(title='Rate Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trip_pairs = (\n",
    "    df.groupby(['PULocationID', 'DOLocationID'])\n",
    "    .size()\n",
    "    .reset_index(name='Trip_Count')\n",
    "    .sort_values(by='Trip_Count', ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# Create a new column for route names\n",
    "trip_pairs['Route'] = trip_pairs['PULocationID'].astype(str) + ' → ' + trip_pairs['DOLocationID'].astype(str)\n",
    "\n",
    "# Plott\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=trip_pairs, x='Route', y='Trip_Count', palette='viridis')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.title('Top 10 Most Frequent Pickup and Drop-off Pairs')\n",
    "plt.xlabel('Route (Pickup → Drop-off)')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = trip_pairs.pivot_table(\n",
    "    index='PULocationID', \n",
    "    columns='DOLocationID', \n",
    "    values='Trip_Count', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, cmap='YlGnBu', annot=True, fmt='.0f', linewidths=0.5, linecolor='gray')\n",
    "plt.title('Heatmap of Top 10 Pickup and Drop-off Pairs')\n",
    "plt.xlabel('Drop-off Location ID')\n",
    "plt.ylabel('Pickup Location ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trip duration in minutes\n",
    "df['trip_duration'] = (df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Group by pickup and dropoff zones and calculate average trip duration\n",
    "longest_trip_zones = (\n",
    "    df.groupby(['PULocationID', 'DOLocationID'])['trip_duration']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(by='trip_duration', ascending=False)\n",
    "    .head(10)  # Show top 10 longest trip pairs\n",
    ")\n",
    "\n",
    "print(\"Top 10 Pickup and Drop-off Zones with Longest Trip Duration:\")\n",
    "print(longest_trip_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari jumlah penumpang yang anomalies\n",
    "unusual_passenger_count = df[(df['passenger_count'] <= 0) | (df['passenger_count'] > 6)]\n",
    "\n",
    "print(\"\\nPerjalanan dengan jumlah penumpang anomalies:\")\n",
    "print(unusual_passenger_count[['PULocationID', 'DOLocationID', 'passenger_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perjalanan dengan pickup dan drop-off di lokasi yang sama, tapi tarif tinggi\n",
    "unusual_zone = df[(df['PULocationID'] == df['DOLocationID']) & (df['fare_amount'] > 50)]\n",
    "\n",
    "print(\"\\nPerjalanan dengan pickup dan drop-off di zona sama tapi tarif tinggi:\")\n",
    "print(unusual_zone[['PULocationID', 'DOLocationID', 'fare_amount', 'trip_duration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perjalanan dengan tarif nol atau negatif\n",
    "unusual_fare = df[df['fare_amount'] <= 0]\n",
    "\n",
    "print(\"\\nPerjalanan dengan tarif nol atau negatif:\")\n",
    "print(unusual_fare[['PULocationID', 'DOLocationID', 'fare_amount', 'trip_distance']])\n",
    "\n",
    "# Perjalanan dengan tarif sangat tinggi tetapi jarak pendek\n",
    "unreasonable_fare = df[(df['fare_amount'] > 200) & (df['trip_distance'] < 5)]\n",
    "\n",
    "print(\"\\nPerjalanan dengan tarif sangat tinggi dan jarak pendek:\")\n",
    "print(unreasonable_fare[['PULocationID', 'DOLocationID', 'fare_amount', 'trip_distance']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melakukan payment analysis\n",
    "\n",
    "payment_analysis = (\n",
    "    df.groupby('payment_type')\n",
    "    .agg(\n",
    "        total_income=('total_amount', 'sum'),  # Total income from each payment type\n",
    "        avg_income_per_trip=('total_amount', 'mean'),  # Average income per trip\n",
    "        total_tip=('tip_amount', 'sum'),  # Total tips\n",
    "        avg_tip_per_trip=('tip_amount', 'mean'),  # Average tip per trip\n",
    "        trip_count=('payment_type', 'size')  # Total number of trips\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by='trip_count', ascending=False)  # Sort by trip count\n",
    ")\n",
    "\n",
    "# Map payment_type codes to meaningful names\n",
    "payment_type_mapping = {\n",
    "    1: 'Credit Card',\n",
    "    2: 'Cash',\n",
    "    3: 'No Charge',\n",
    "    4: 'Dispute',\n",
    "    5: 'Unknown',\n",
    "    6: 'Voided Trip'\n",
    "}\n",
    "payment_analysis['payment_type'] = payment_analysis['payment_type'].map(payment_type_mapping)\n",
    "\n",
    "print(\"Payment Method Analysis:\")\n",
    "print(payment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melakukan kalulasi trip dalam menit\n",
    "df['trip_duration'] = (df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Group by VendorID \n",
    "vendor_performance = (\n",
    "    df.groupby('VendorID')\n",
    "    .agg(\n",
    "        avg_trip_duration=('trip_duration', 'mean'),\n",
    "        avg_trip_distance=('trip_distance', 'mean'),\n",
    "        total_trips=('VendorID', 'size'),\n",
    "        avg_fare_amount=('fare_amount', 'mean'),\n",
    "        avg_tip_amount=('tip_amount', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Vendor Performance Comparison:\")\n",
    "print(vendor_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "**Gunakan melb_data.csv (Melbourne House Data) dengan:**\n",
    "Do the rides that originate from certain areas tend to generate more revenue?\n",
    "Does the distance of a trip considerably impact the received tip?\n",
    "Does the average fare differ with modes of payment, such as cash versus credit card?\n",
    "1. Does Time of Day Impact Fare Amounts?\n",
    "Hypothesis: The objective is to test if there is a significant difference between the amount of fares at different parts of the day-for example, morning and evening.\n",
    "Example:\n",
    "H₀: The mean amount of fare is the same for different times of day.\n",
    "H₁: There is a significant difference in the average fare amount by time of day, such as rush hours versus off-peak hours.\n",
    "This type of analysis could be helpful in determining the best times to make more drivers available in order to increase revenues.\n",
    "\n",
    "2. Is There Seasonal Variation in Trip Demand?\n",
    "Hypothesis: Test for a significant difference in the number of trips across seasons or months.\n",
    "Example:\n",
    "H₀: The number of trips is equal across all seasons.\n",
    "H₁: There is some significant difference in the number of trips across seasons.\n",
    "If you find seasonality, that could be helpful to adjust fleet size or marketing effort in high demand periods.\n",
    "\n",
    "3. Is the Number of Passengers Influencing the Fare Amount?\n",
    "Hypothesis: Check if larger groups are willing to pay more, or if the number of passengers influences the fare amount pricing.\n",
    "Example:\n",
    "H₀: Number of passengers and fare amount are independent.\n",
    "H₁: There is a significant difference in fare amount depending on the number of passengers.\n",
    "This could be enlightening about shared vs. individual ride fare strategies.\n",
    " \n",
    "4. Does the Tip Amount Differ Due to Payment Type?\n",
    "Hypothesis: Use a t-test to determine if credit card vs. cash determines different tip amounts.\n",
    "Example:\n",
    "H₀: The average fare tip is equal for credit card payments as well as cash payments.\n",
    "H₁: Credit card and cash are yielding significantly different average tip amounts.\n",
    "Knowing these patterns in tipping due to payment type may provide an indication of how to incentivize certain types of payments.\n",
    "\n",
    "5. Is Trip Distance a More Important Predictor of Total Amounts Compared with Other Variables?\n",
    "Hypothesis: Determine if trip distance is most influential with respect to total trip revenue compared with other variables such as duration of trip or rate code.\n",
    "Example:\n",
    "Ho: Trip distance is no more influential determinant of total amount compared to other factors.\n",
    "H1: Trip distance is a more influential determinant of the total amount compared to other factors.\n",
    "The test described will support or reject the supposition that longer trips will always generate more money and therefore shape the pricing and distance strategies.\n",
    "\n",
    "6. Are Fares from Airport Locations Significantly Higher Than Non-Airport Locations?\n",
    "Hypothesis: Determine if the average fares for trips that begin or end in major airport locations, such as JFK or Newark, are higher compared to others. Example: H₀: Average fares in airport and non-airport locations are equal. H₁: Average fares for airport locations are much higher. This will be useful insight that feeds targeted services or dynamic pricing models for airport trips.\n",
    "\n",
    "7. Do Store-and-Forward Trips (Disconnected Trips) Differ in Revenue?\n",
    "Hypothesis-Check whether the trips which were flagged as store-and-forward due to temporary network disconnects, generate different revenues compared to regular trips. Example: H0 : the average fare amount of the store-and-forward trips is equal to the fare amount in regular trips. H1: there is a significant difference between store-and-forward trips and regular trips in terms of fare amount. This might show if the network connectivity is affecting the value of fares, that could have an implication in the collection process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Difference in Average Fare by Payment Type\n",
    "This test will check if payment type impacts the average fare amount.\n",
    "\n",
    "H₀: The average fare is the same for all payment types.\n",
    "H₁: There is a significant difference in average fare between at least two payment types.\n",
    "Code Example for ANOVA Test:\n",
    "python\n",
    "Copy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Filter dataset for valid payment types\n",
    "data_filtered = df[df['Payment_type'].isin([1, 2])]  \n",
    "\n",
    "# Extract fare amounts by payment type\n",
    "fare_credit = data_filtered[data_filtered['Payment_type'] == 1]['Fare_amount']\n",
    "fare_cash = data_filtered[data_filtered['Payment_type'] == 2]['Fare_amount']\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = stats.f_oneway(fare_credit, fare_cash)\n",
    "print(\"ANOVA F-statistic:\", f_stat)\n",
    "print(\"ANOVA p-value:\", p_value)\n",
    "\n",
    "# Interpret the p-value\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: Significant difference in average fare by payment type.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference in average fare by payment type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2: Impact of Trip Distance on Tip Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pearson correlation test\n",
    "correlation, p_value = stats.pearsonr(df['Trip_distance'], df['Tip_amount'])\n",
    "print(\"Correlation coefficient:\", correlation)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the p-value\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: Significant correlation between trip distance and tip amount.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant correlation between trip distance and tip amount.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Test 3: Do High-Volume Zones Generate Higher Total Revenue?\n",
    "This test will check if there’s a significant difference in total revenue for trips originating in high-demand zones versus low-demand zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define high-demand zones (top 10% by trip count)\n",
    "top_zones = df['PULocationID'].value_counts().head(int(len(data) * 0.1)).index\n",
    "df['is_high_demand'] = df['PULocationID'].apply(lambda x: 1 if x in top_zones else 0)\n",
    "\n",
    "# Separate data into high-demand and low-demand zone trips\n",
    "high_demand_fares = df[df['is_high_demand'] == 1]['Total_amount']\n",
    "low_demand_fares = df[df['is_high_demand'] == 0]['Total_amount']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(high_demand_fares, low_demand_fares, equal_var=False)\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the p-value\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: High-demand zones have significantly different total revenue.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference in total revenue between high and low-demand zones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Statistical Inference and Confidence Intervals\n",
    "For added robustness, calculate confidence intervals for mean fare or revenue in high-demand vs. low-demand zones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95% confidence interval for high-demand zones\n",
    "high_demand_mean = high_demand_fares.mean()\n",
    "high_demand_std = high_demand_fares.std()\n",
    "high_demand_n = len(high_demand_fares)\n",
    "high_demand_conf_interval = stats.norm.interval(0.95, loc=high_demand_mean, scale=high_demand_std / np.sqrt(high_demand_n))\n",
    "\n",
    "print(\"95% Confidence Interval for High-Demand Zone Revenue:\", high_demand_conf_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
